{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = '/Users/foochuanyue/Desktop/aiap-14/all-assignments/assignment7/aiap-14-ds-llm-topic-01-sa.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from typing import List\n",
    "from csv import writer\n",
    "import sys\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from langchain.document_loaders import CSVLoader\n",
    "from langchain.embeddings import VertexAIEmbeddings\n",
    "from langchain.llms import VertexAI\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from vertexai.preview.language_models import TextGenerationModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from src.generative_agents.generative_agent import StemssGenerativeAgent\n",
    "from src.generative_agents.memory import StemssGenerativeAgentMemory\n",
    "from src.retrievers.time_weighted_retriever import ModTimeWeightedVectorStoreRetriever\n",
    "from src.vectorstores.chroma import EnhancedChroma\n",
    "from src.generators.schedule import generate_schedule\n",
    "from src.generators.agent import generate_agent_name, generate_characters\n",
    "from src.tools.action import force_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents() -> List[Document]:\n",
    "    \"\"\" \"Load memory (history) from CSV \"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of documents where each row of the CSV is a document\n",
    "    \"\"\"\n",
    "    loader = CSVLoader(\n",
    "        mem_file, metadata_columns=[\"last_accessed_at\", \"created_at\", \"importance\"]\n",
    "    )\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "def create_new_memory_retriever(decay_rate: float = 0.5, k: int = 5, mem_file:str=\"./memory/memory.csv\"):\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Define your embedding model\n",
    "    embeddings_model = VertexAIEmbeddings()\n",
    "    vs = EnhancedChroma(embedding_function=embeddings_model)\n",
    "    return ModTimeWeightedVectorStoreRetriever(\n",
    "        vectorstore=vs, other_score_keys=[\"importance\"], decay_rate=decay_rate, k=k, mem_file=mem_file\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the path of Google Cloud credentials from the environment variable\n",
    "google_creds_path = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='../outputs'\n",
    "all_folders = os.listdir(path)\n",
    "new = 1\n",
    "if len(all_folders)!=0:\n",
    "    all_folders.sort()\n",
    "    latest = all_folders[-1].replace('run_', '')\n",
    "    new = int(latest) + 1\n",
    "new_path = f\"{path}/run_{new}\"\n",
    "os.makedirs(new_path)\n",
    "os.makedirs(f\"{new_path}/memory\")\n",
    "\n",
    "\n",
    "agents = []\n",
    "#Generate agent\n",
    "llm = VertexAI(model_name=\"text-bison@001\", max_output_tokens=256, temperature=0.2)\n",
    "agent_names = generate_agent_name(model=llm, num_of_agents=2)\n",
    "for agent_name in agent_names:\n",
    "\n",
    "    llm = VertexAI(model_name=\"text-bison@001\", max_output_tokens=256, temperature=0.7)\n",
    "    #generate agent details\n",
    "    agent_details = generate_characters(model=llm, agent_name=agent_name)\n",
    "    agent_details['name']=agent_name\n",
    "\n",
    "    #Create csv if it doesn't exist\n",
    "    mem_file = f\"{new_path}/memory/{agent_name}.csv\"\n",
    "    if  not os.path.exists(mem_file):\n",
    "        with open(mem_file, 'a') as f_object:\n",
    "            List = [\"created_at\", \"last_accessed_at\", \"observations\", \"importance\"]\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(List)\n",
    "            f_object.close()\n",
    "\n",
    "    #Load CSV\n",
    "    docs = load_documents()\n",
    "    memory_retriever = create_new_memory_retriever(decay_rate=0.5, k=5 ,mem_file=mem_file)\n",
    "    if(len(docs)!=0):\n",
    "        memory_retriever.add_documents(docs)\n",
    "\n",
    "    agent_memory = StemssGenerativeAgentMemory(\n",
    "        llm=llm,\n",
    "        memory_retriever=memory_retriever,\n",
    "        reflection_threshold=8,  # we will give this a relatively low number to show how reflection works\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    agent = StemssGenerativeAgent(\n",
    "        name=agent_name,\n",
    "        age=agent_details[\"age\"],\n",
    "        traits=agent_details[\"traits\"],\n",
    "        status=\"\",\n",
    "        memory_retriever=create_new_memory_retriever(),\n",
    "        llm=llm,\n",
    "        memory=agent_memory,\n",
    "        background=agent_details[\"background\"],\n",
    "    )\n",
    "    agent.get_summary()\n",
    "    agents.append(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mOn the scale of 1 to 10, where 1 is not important at all (e.g., brushing teeth, making bed) and 10 is extremely important (e.g., a break up, college acceptance), rate the importance of the following piece of memory. You must respond with a single integer.\n",
      "Memory: Alex observed Plan to finish the pending conversation with your spouse at home\n",
      "Rating: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importance score: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mOn the scale of 1 to 10, where 1 is not important at all (e.g., brushing teeth, making bed) and 10 is extremely important (e.g., a break up, college acceptance), rate the importance of the following piece of memory. You must respond with a single integer.\n",
      "Memory: Alex observed Plan to have a talk with your spouse about the upcoming meeting\n",
      "Rating: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importance score: 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "observations = generate_schedule(model=llm, agent=agents[0])\n",
    "for observation in observations:\n",
    "    agents[0].add_memory(observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Alex (age: 25)\\nInnate traits: ambitious, charming, impatient\\nAlex is passionate and driven to help others, but she can sometimes be impatient.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Ben (age: 32)\\nInnate traits: curious, shy, optimistic\\nBen is a shy and introverted librarian who loves to read and help others.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[1].get_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alex (age: 25)\n",
      "Innate traits: ambitious, charming, impatient\n",
      "Alex is passionate and driven to help others, but she can sometimes be impatient.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 100 is greater than number of elements in index 2, updating n_results = 2\n",
      "Number of requested results 100 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alex (age: 25)\n",
      "Innate traits: ambitious, charming, impatient\n",
      "Alex is passionate and driven to help others, but she can sometimes be impatient.\n",
      "It is October 14, 2023, 06:06 PM.\n",
      "Alex's status: \n",
      "Summary of relevant context from Alex's memory:\n",
      "The correct answer is: 'creator'.\n",
      "\n",
      "In the context, Alex is the creator of the two plans.\n",
      "Most recent observations: []\n",
      "Observation: Hello\n",
      "\n",
      "What would Alex say? If the conversation is getting similar end it. To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mOn the scale of 1 to 10, where 1 is not important at all (e.g., brushing teeth, making bed) and 10 is extremely important (e.g., a break up, college acceptance), rate the importance of the following piece of memory. You must respond with a single integer.\n",
      "Memory: Alex observed Hello and said Hi. What can I help you with?\n",
      "Rating: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importance score: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'Alex said Hi. What can I help you with?')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[0].generate_dialogue_response(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Ben (age: 32)\n",
      "Innate traits: curious, shy, optimistic\n",
      "Ben is a shy and introverted librarian who loves to read and help others.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Ben (age: 32)\n",
      "Innate traits: curious, shy, optimistic\n",
      "Ben is a shy and introverted librarian who loves to read and help others.\n",
      "It is October 14, 2023, 06:05 PM.\n",
      "Ben's status: \n",
      "Summary of relevant context from Ben's memory:\n",
      "Ben is the name of a person. Ben is a person who greets others. Hello is an interjection used as a greeting or salutation. Ben uses hello to greet others. So the relationship between Ben and hello is that hello is used by Ben to greet others.\n",
      "Most recent observations: []\n",
      "Observation: Hello\n",
      "\n",
      "What would Ben say? If the conversation is getting similar end it. To end the conversation, write: GOODBYE: \"what to say\". Otherwise to continue the conversation, write: SAY: \"what to say next\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mOn the scale of 1 to 10, where 1 is not important at all (e.g., brushing teeth, making bed) and 10 is extremely important (e.g., a break up, college acceptance), rate the importance of the following piece of memory. You must respond with a single integer.\n",
      "Memory: Ben observed Hello and said Hello to you too. What can I help you with?\n",
      "Rating: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importance score: 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 'Ben said Hello to you too. What can I help you with?')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agents[1].generate_dialogue_response(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mOn the scale of 1 to 10, where 1 is not important at all (e.g., brushing teeth, making bed) and 10 is extremely important (e.g., a break up, college acceptance), rate the importance of the following piece of memory. You must respond with a single integer.\n",
      "Memory: Bob observed test\n",
      "Rating: \u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importance score: 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agents[1].add_memory('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Alex (age: 23)\n",
      "Innate traits: cynical, sarcastic, smart\n",
      "Alex is a recent college graduate working as a software engineer at a local startup. He is a bit of a loner and doesn't really have any close friends in AI Town yet. He is very focused on his work and enjoys starting his day early with a cup of coffee and checking his email.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/foochuanyue/Desktop/aiap-14/stems-mini-project/notebooks/action-agent.ipynb Cell 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/foochuanyue/Desktop/aiap-14/stems-mini-project/notebooks/action-agent.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m force_dialogue(agents[\u001b[39m0\u001b[39;49m], agents[\u001b[39m1\u001b[39;49m])\n",
      "File \u001b[0;32m~/Desktop/aiap-14/stems-mini-project/notebooks/../src/tools/action.py:3\u001b[0m, in \u001b[0;36mforce_dialogue\u001b[0;34m(agent1, agent2)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforce_dialogue\u001b[39m(agent1, agent2):\n\u001b[1;32m      2\u001b[0m     conversation_history \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m     response2 \u001b[39m=\u001b[39m agent1\u001b[39m.\u001b[39;49mgenerate_dialogue_response(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mSay sometihng to \u001b[39;49m\u001b[39m{\u001b[39;49;00magent2\u001b[39m.\u001b[39;49mname\u001b[39m}\u001b[39;49;00m\u001b[39m about your day\u001b[39;49m\u001b[39m\"\u001b[39;49m, conversation_history\u001b[39m=\u001b[39;49mconversation_history)\n\u001b[1;32m      4\u001b[0m     response1 \u001b[39m=\u001b[39m speak_to(agent2, response2[\u001b[39m1\u001b[39m], conversation_history\u001b[39m=\u001b[39mconversation_history)\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(response1[\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m~/Desktop/aiap-14/stems-mini-project/notebooks/../src/generative_agents/generative_agent.py:198\u001b[0m, in \u001b[0;36mStemssGenerativeAgent.generate_dialogue_response\u001b[0;34m(self, observation, now, conversation_history)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"React to a given observation.\"\"\"\u001b[39;00m\n\u001b[1;32m    193\u001b[0m call_to_action_template \u001b[39m=\u001b[39m (\n\u001b[1;32m    194\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mWhat would \u001b[39m\u001b[39m{agent_name}\u001b[39;00m\u001b[39m say? If the conversation is getting similar end it. To end the conversation, write:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m GOODBYE: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhat to say\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m. Otherwise to continue the conversation,\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    196\u001b[0m     \u001b[39m'\u001b[39m\u001b[39m write: SAY: \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mwhat to say next\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[1;32m    197\u001b[0m )\n\u001b[0;32m--> 198\u001b[0m full_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_reaction(\n\u001b[1;32m    199\u001b[0m     observation, call_to_action_template, now\u001b[39m=\u001b[39;49mnow, conversation_history\u001b[39m=\u001b[39;49mconversation_history\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    201\u001b[0m result \u001b[39m=\u001b[39m full_result\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    202\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mGOODBYE:\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m result:\n",
      "File \u001b[0;32m~/Desktop/aiap-14/stems-mini-project/notebooks/../src/generative_agents/generative_agent.py:166\u001b[0m, in \u001b[0;36mStemssGenerativeAgent._generate_reaction\u001b[0;34m(self, observation, suffix, now, conversation_history)\u001b[0m\n\u001b[1;32m    164\u001b[0m agent_summary_description \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_summary(now\u001b[39m=\u001b[39mnow)\n\u001b[1;32m    165\u001b[0m \u001b[39mprint\u001b[39m(agent_summary_description)\n\u001b[0;32m--> 166\u001b[0m relevant_memories_str \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msummarize_related_memories(observation)\n\u001b[1;32m    167\u001b[0m current_time_str \u001b[39m=\u001b[39m (\n\u001b[1;32m    168\u001b[0m     datetime\u001b[39m.\u001b[39mnow()\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mB \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY, \u001b[39m\u001b[39m%\u001b[39m\u001b[39mI:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM \u001b[39m\u001b[39m%\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    169\u001b[0m     \u001b[39mif\u001b[39;00m now \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39melse\u001b[39;00m now\u001b[39m.\u001b[39mstrftime(\u001b[39m\"\u001b[39m\u001b[39m%\u001b[39m\u001b[39mB \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m%\u001b[39m\u001b[39mY, \u001b[39m\u001b[39m%\u001b[39m\u001b[39mI:\u001b[39m\u001b[39m%\u001b[39m\u001b[39mM \u001b[39m\u001b[39m%\u001b[39m\u001b[39mp\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m )\n\u001b[1;32m    172\u001b[0m kwargs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    173\u001b[0m     agent_summary_description\u001b[39m=\u001b[39magent_summary_description,\n\u001b[1;32m    174\u001b[0m     current_time\u001b[39m=\u001b[39mcurrent_time_str,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    179\u001b[0m     most_recent_memories\u001b[39m=\u001b[39mconversation_history\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/aiap-14/stems-mini-project/notebooks/../src/generative_agents/generative_agent.py:47\u001b[0m, in \u001b[0;36mStemssGenerativeAgent.summarize_related_memories\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     43\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(\n\u001b[1;32m     44\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39m{q1}\u001b[39;00m\u001b[39m?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mContext from memory:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m{relevant_memories}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mRelevant context:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m )\n\u001b[1;32m     46\u001b[0m entity_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_entity_from_observation(observation)\n\u001b[0;32m---> 47\u001b[0m entity_action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_entity_action(observation, entity_name)\n\u001b[1;32m     48\u001b[0m q1 \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is the relationship between \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00mentity_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m q2 \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mentity_name\u001b[39m}\u001b[39;00m\u001b[39m is \u001b[39m\u001b[39m{\u001b[39;00mentity_action\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/aiap-14/stems-mini-project/notebooks/../src/generative_agents/generative_agent.py:39\u001b[0m, in \u001b[0;36mStemssGenerativeAgent._get_entity_action\u001b[0;34m(self, observation, entity_name)\u001b[0m\n\u001b[1;32m     34\u001b[0m instruction \u001b[39m=\u001b[39m (\n\u001b[1;32m     35\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mWhat is the \u001b[39m\u001b[39m{\u001b[39;00mentity_name\u001b[39m}\u001b[39;00m\u001b[39m doing in the following observation?\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mObservation: \u001b[39m\u001b[39m{\u001b[39;00mobservation\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     37\u001b[0m )\n\u001b[1;32m     38\u001b[0m prompt \u001b[39m=\u001b[39m PromptTemplate\u001b[39m.\u001b[39mfrom_template(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msystem_prompt \u001b[39m%\u001b[39m instruction)\n\u001b[0;32m---> 39\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchain(prompt)\u001b[39m.\u001b[39;49mrun(entity\u001b[39m=\u001b[39;49mentity_name, observation\u001b[39m=\u001b[39;49mobservation)\u001b[39m.\u001b[39mstrip()\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/chains/base.py:506\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    502\u001b[0m         _output_key\n\u001b[1;32m    503\u001b[0m     ]\n\u001b[1;32m    505\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 506\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    507\u001b[0m         _output_key\n\u001b[1;32m    508\u001b[0m     ]\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    511\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    512\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    514\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/chains/base.py:306\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    305\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 306\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    307\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    308\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    309\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    310\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/chains/base.py:300\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    293\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    294\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    295\u001b[0m     inputs,\n\u001b[1;32m    296\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    297\u001b[0m )\n\u001b[1;32m    298\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 300\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    301\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    302\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    303\u001b[0m     )\n\u001b[1;32m    304\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    305\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/chains/llm.py:93\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     89\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     90\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     91\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     92\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 93\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     94\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/chains/llm.py:103\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    102\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m--> 103\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m    104\u001b[0m     prompts,\n\u001b[1;32m    105\u001b[0m     stop,\n\u001b[1;32m    106\u001b[0m     callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    107\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_kwargs,\n\u001b[1;32m    108\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/base.py:498\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    491\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    495\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    496\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    497\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 498\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/base.py:647\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    633\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    634\u001b[0m         )\n\u001b[1;32m    635\u001b[0m     run_managers \u001b[39m=\u001b[39m [\n\u001b[1;32m    636\u001b[0m         callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    637\u001b[0m             dumpd(\u001b[39mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    645\u001b[0m         )\n\u001b[1;32m    646\u001b[0m     ]\n\u001b[0;32m--> 647\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_helper(\n\u001b[1;32m    648\u001b[0m         prompts, stop, run_managers, \u001b[39mbool\u001b[39;49m(new_arg_supported), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    649\u001b[0m     )\n\u001b[1;32m    650\u001b[0m     \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m    651\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(missing_prompts) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/base.py:535\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n\u001b[1;32m    534\u001b[0m         run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 535\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    536\u001b[0m flattened_outputs \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m    537\u001b[0m \u001b[39mfor\u001b[39;00m manager, flattened_output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/base.py:522\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_generate_helper\u001b[39m(\n\u001b[1;32m    513\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    514\u001b[0m     prompts: List[\u001b[39mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    518\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m    519\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    520\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    521\u001b[0m         output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 522\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(\n\u001b[1;32m    523\u001b[0m                 prompts,\n\u001b[1;32m    524\u001b[0m                 stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m    525\u001b[0m                 \u001b[39m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    526\u001b[0m                 run_manager\u001b[39m=\u001b[39;49mrun_managers[\u001b[39m0\u001b[39;49m] \u001b[39mif\u001b[39;49;00m run_managers \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    527\u001b[0m                 \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    528\u001b[0m             )\n\u001b[1;32m    529\u001b[0m             \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    530\u001b[0m             \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    531\u001b[0m         )\n\u001b[1;32m    532\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    533\u001b[0m         \u001b[39mfor\u001b[39;00m run_manager \u001b[39min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/vertexai.py:285\u001b[0m, in \u001b[0;36mVertexAI._generate\u001b[0;34m(self, prompts, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    283\u001b[0m         generations\u001b[39m.\u001b[39mappend([generation])\n\u001b[1;32m    284\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 285\u001b[0m         res \u001b[39m=\u001b[39m completion_with_retry(\n\u001b[1;32m    286\u001b[0m             \u001b[39mself\u001b[39;49m, prompt, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[1;32m    287\u001b[0m         )\n\u001b[1;32m    288\u001b[0m         generations\u001b[39m.\u001b[39mappend([_response_to_generation(res)])\n\u001b[1;32m    289\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/vertexai.py:102\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[0;34m(llm, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    100\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mpredict(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/tenacity/__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[1;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[0;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/tenacity/__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[1;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[1;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/tenacity/__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[1;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[0;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[1;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/concurrent/futures/_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    450\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 451\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    453\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/tenacity/__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[1;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[1;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/langchain/llms/vertexai.py:100\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[1;32m     99\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39margs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m--> 100\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mpredict(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/vertexai/language_models/_language_models.py:774\u001b[0m, in \u001b[0;36m_TextGenerationModel.predict\u001b[0;34m(self, prompt, max_output_tokens, temperature, top_k, top_p, stop_sequences, candidate_count)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Gets model response for a single prompt.\u001b[39;00m\n\u001b[1;32m    751\u001b[0m \n\u001b[1;32m    752\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[39m    A `MultiCandidateTextGenerationResponse` object that contains the text produced by the model.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    764\u001b[0m prediction_request \u001b[39m=\u001b[39m _create_text_generation_prediction_request(\n\u001b[1;32m    765\u001b[0m     prompt\u001b[39m=\u001b[39mprompt,\n\u001b[1;32m    766\u001b[0m     max_output_tokens\u001b[39m=\u001b[39mmax_output_tokens,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    771\u001b[0m     candidate_count\u001b[39m=\u001b[39mcandidate_count,\n\u001b[1;32m    772\u001b[0m )\n\u001b[0;32m--> 774\u001b[0m prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_endpoint\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m    775\u001b[0m     instances\u001b[39m=\u001b[39;49m[prediction_request\u001b[39m.\u001b[39;49minstance],\n\u001b[1;32m    776\u001b[0m     parameters\u001b[39m=\u001b[39;49mprediction_request\u001b[39m.\u001b[39;49mparameters,\n\u001b[1;32m    777\u001b[0m )\n\u001b[1;32m    779\u001b[0m \u001b[39mreturn\u001b[39;00m _parse_text_generation_model_multi_candidate_response(prediction_response)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/google/cloud/aiplatform/models.py:1596\u001b[0m, in \u001b[0;36mEndpoint.predict\u001b[0;34m(self, instances, parameters, timeout, use_raw_predict)\u001b[0m\n\u001b[1;32m   1583\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1584\u001b[0m         predictions\u001b[39m=\u001b[39mjson_response[\u001b[39m\"\u001b[39m\u001b[39mpredictions\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1585\u001b[0m         deployed_model_id\u001b[39m=\u001b[39mraw_predict_response\u001b[39m.\u001b[39mheaders[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m         ),\n\u001b[1;32m   1594\u001b[0m     )\n\u001b[1;32m   1595\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1596\u001b[0m     prediction_response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_prediction_client\u001b[39m.\u001b[39;49mpredict(\n\u001b[1;32m   1597\u001b[0m         endpoint\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gca_resource\u001b[39m.\u001b[39;49mname,\n\u001b[1;32m   1598\u001b[0m         instances\u001b[39m=\u001b[39;49minstances,\n\u001b[1;32m   1599\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[1;32m   1600\u001b[0m         timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m   1601\u001b[0m     )\n\u001b[1;32m   1603\u001b[0m     \u001b[39mreturn\u001b[39;00m Prediction(\n\u001b[1;32m   1604\u001b[0m         predictions\u001b[39m=\u001b[39m[\n\u001b[1;32m   1605\u001b[0m             json_format\u001b[39m.\u001b[39mMessageToDict(item)\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1610\u001b[0m         model_resource_name\u001b[39m=\u001b[39mprediction_response\u001b[39m.\u001b[39mmodel,\n\u001b[1;32m   1611\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/google/cloud/aiplatform_v1/services/prediction_service/client.py:604\u001b[0m, in \u001b[0;36mPredictionServiceClient.predict\u001b[0;34m(self, request, endpoint, instances, parameters, retry, timeout, metadata)\u001b[0m\n\u001b[1;32m    599\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(metadata) \u001b[39m+\u001b[39m (\n\u001b[1;32m    600\u001b[0m     gapic_v1\u001b[39m.\u001b[39mrouting_header\u001b[39m.\u001b[39mto_grpc_metadata(((\u001b[39m\"\u001b[39m\u001b[39mendpoint\u001b[39m\u001b[39m\"\u001b[39m, request\u001b[39m.\u001b[39mendpoint),)),\n\u001b[1;32m    601\u001b[0m )\n\u001b[1;32m    603\u001b[0m \u001b[39m# Send the request.\u001b[39;00m\n\u001b[0;32m--> 604\u001b[0m response \u001b[39m=\u001b[39m rpc(\n\u001b[1;32m    605\u001b[0m     request,\n\u001b[1;32m    606\u001b[0m     retry\u001b[39m=\u001b[39;49mretry,\n\u001b[1;32m    607\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout,\n\u001b[1;32m    608\u001b[0m     metadata\u001b[39m=\u001b[39;49mmetadata,\n\u001b[1;32m    609\u001b[0m )\n\u001b[1;32m    611\u001b[0m \u001b[39m# Done; return the response.\u001b[39;00m\n\u001b[1;32m    612\u001b[0m \u001b[39mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/google/api_core/gapic_v1/method.py:131\u001b[0m, in \u001b[0;36m_GapicCallable.__call__\u001b[0;34m(self, timeout, retry, compression, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compression \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    129\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mcompression\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m compression\n\u001b[0;32m--> 131\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped_func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/google/api_core/grpc_helpers.py:75\u001b[0m, in \u001b[0;36m_wrap_unary_errors.<locals>.error_remapped_callable\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(callable_)\n\u001b[1;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39merror_remapped_callable\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     74\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m         \u001b[39mreturn\u001b[39;00m callable_(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     76\u001b[0m     \u001b[39mexcept\u001b[39;00m grpc\u001b[39m.\u001b[39mRpcError \u001b[39mas\u001b[39;00m exc:\n\u001b[1;32m     77\u001b[0m         \u001b[39mraise\u001b[39;00m exceptions\u001b[39m.\u001b[39mfrom_grpc_error(exc) \u001b[39mfrom\u001b[39;00m \u001b[39mexc\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/grpc/_channel.py:1158\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1147\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1148\u001b[0m     request: Any,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     compression: Optional[grpc\u001b[39m.\u001b[39mCompression] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1154\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m   1155\u001b[0m     (\n\u001b[1;32m   1156\u001b[0m         state,\n\u001b[1;32m   1157\u001b[0m         call,\n\u001b[0;32m-> 1158\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_blocking(\n\u001b[1;32m   1159\u001b[0m         request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[1;32m   1160\u001b[0m     )\n\u001b[1;32m   1161\u001b[0m     \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/mini-project/lib/python3.10/site-packages/grpc/_channel.py:1142\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable._blocking\u001b[0;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[1;32m   1126\u001b[0m state\u001b[39m.\u001b[39mmethod \u001b[39m=\u001b[39m _common\u001b[39m.\u001b[39mdecode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_method)\n\u001b[1;32m   1127\u001b[0m call \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_channel\u001b[39m.\u001b[39msegregated_call(\n\u001b[1;32m   1128\u001b[0m     cygrpc\u001b[39m.\u001b[39mPropagationConstants\u001b[39m.\u001b[39mGRPC_PROPAGATE_DEFAULTS,\n\u001b[1;32m   1129\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1140\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_context,\n\u001b[1;32m   1141\u001b[0m )\n\u001b[0;32m-> 1142\u001b[0m event \u001b[39m=\u001b[39m call\u001b[39m.\u001b[39;49mnext_event()\n\u001b[1;32m   1143\u001b[0m _handle_event(event, state, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_response_deserializer)\n\u001b[1;32m   1144\u001b[0m \u001b[39mreturn\u001b[39;00m state, call\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:366\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc.SegregatedCall.next_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:187\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/channel.pyx.pxi:181\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next_call_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:78\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:61\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._internal_latent_event\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/python/grpcio/grpc/_cython/_cygrpc/completion_queue.pyx.pxi:42\u001b[0m, in \u001b[0;36mgrpc._cython.cygrpc._next\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "force_dialogue(agents[0], agents[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mini-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
